{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed7d190",
   "metadata": {},
   "source": [
    "### 0. Getting the right folder and files & import statements ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfad2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "#from ultralytics import YOLO #obj detection algorithm\n",
    "import cv2 # videos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os #to get files automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3266c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frame timestamps data loaded from: /Users/alina/Downloads/Thesis/raw/Participant 16/P16_2/world_timestamps.csv\n",
      "Detections data loaded from: /Users/alina/Downloads/Thesis/raw/Participant 16/P16_2/detections_16_2.csv\n",
      "Fixations data loaded from: /Users/alina/Downloads/Thesis/raw/Participant 16/P16_2/fixations.csv\n",
      "Face data loaded from: /Users/alina/Downloads/Thesis/raw/Participant 16/P16_2/fixations_on_face.csv\n"
     ]
    }
   ],
   "source": [
    "# Get participant number input\n",
    "participant_number = input(\"Enter participant number: \")\n",
    "\n",
    "# Define the base path\n",
    "base_path = \"/Users/alina/Downloads/Thesis/raw/\"\n",
    "\n",
    "# Build the folder path for the participant\n",
    "folder_path = os.path.join(base_path, f\"Participant {participant_number}\")\n",
    "\n",
    "# Get recording number input\n",
    "recording_number = input(\"Enter recording number: \")\n",
    "\n",
    "# Build the recording folder path\n",
    "recording_path = os.path.join(folder_path, f\"P{participant_number}_{recording_number}\")\n",
    "\n",
    "# Read the CSV files with dynamic paths\n",
    "world_timestamps_file = os.path.join(recording_path, \"world_timestamps.csv\")\n",
    "detections_file = os.path.join(recording_path, f\"detections_{participant_number}_{recording_number}.csv\")\n",
    "\n",
    "fixations_file = os.path.join(recording_path, \"fixations.csv\")\n",
    "face_file = os.path.join(recording_path, \"fixations_on_face.csv\")\n",
    "#saccades_file = os.path.join(recording_path, \"saccades.csv\")\n",
    "\n",
    "# Load the data\n",
    "world_timestamps_df = pd.read_csv(world_timestamps_file)\n",
    "detections_df = pd.read_csv(detections_file)\n",
    "fixations_df = pd.read_csv(fixations_file)\n",
    "face_df = pd.read_csv(face_file)\n",
    "#saccades_df = pd.read_csv(saccades_file)\n",
    "\n",
    "print(f\"Video frame timestamps data loaded from: {world_timestamps_file}\")\n",
    "print(f\"Detections data loaded from: {detections_file}\")\n",
    "#print(f\"Gaze data loaded from: {gaze_file}\")\n",
    "print(f\"Fixations data loaded from: {fixations_file}\")\n",
    "print(f\"Face data loaded from: {face_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641f28f",
   "metadata": {},
   "source": [
    "## Run yolo (not on laptop) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bac2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Annotating video and making a detections file (Lab's computer) ###\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# input video\n",
    "video_path = \"/Users/alina/Downloads/Thesis/Python notebooks/2.mp4\"  # Replace with needed video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# input video information\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total number of frames\n",
    "fr = cap.get(cv2.CAP_PROP_FPS)  # Frame rate (frames per second)\n",
    "video_duration = frame_count / fr  # Duration in seconds\n",
    "\n",
    "print(f\"Total frames: {frame_count}\")\n",
    "print(f\"Frame rate: {fr} FPS\")\n",
    "print(f\"Video duration: {video_duration:.2f} seconds\")\n",
    "\n",
    "# for output video (annotated video)\n",
    "output_path = \"/Users/alina/Downloads/Thesis/Python notebooks/2yolo.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(output_path, fourcc, fr, (int(cap.get(3)), int(cap.get(4)))) #making sure the output video has same attributes as the input\n",
    "\n",
    "\n",
    "## detections df\n",
    "detections = [] # a list to for detection results\n",
    "frame_count = 0 # initialising frame count\n",
    "\n",
    "while cap.isOpened(): # a loop getting frames until the video is finished\n",
    "    ret, frame = cap.read() \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1  # Track the frame number\n",
    "\n",
    "    # Run YOLOv8 detection on the current frame\n",
    "    results = model(frame,classes=0)\n",
    "\n",
    "    # Save detections to the list\n",
    "    for result in results[0].boxes.data:\n",
    "        x_min, y_min, x_max, y_max, confidence, class_id = result\n",
    "        class_label = model.names[int(class_id)]  # get class name from class ID\n",
    "        detections.append([frame_count, x_min.item(), y_min.item(), x_max.item(), y_max.item(), confidence.item(), class_label])\n",
    "\n",
    "    # Annotate frame with bounding boxes and save to output video\n",
    "    annotated_frame = results[0].plot()\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    ##  to display the annotated frame during the process\n",
    "    '''cv2.imshow(\"YOLO Detection\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break'''\n",
    "\n",
    "# making a df\n",
    "columns = [\"Frame\", \"X_min\", \"Y_min\", \"X_max\", \"Y_max\", \"Confidence\", \"Class\"]\n",
    "detections_df = pd.DataFrame(detections, columns=columns)\n",
    "saving_path = os.path.join(recording_path, f\"detections_{participant_number}_{recording_number}.csv\")\n",
    "detections_df.to_csv(saving_path, index=False)\n",
    "\n",
    "\n",
    "cap.release() # closes the video file\n",
    "out.release() # closes the video writer\n",
    "cv2.destroyAllWindows() # closes any OpenCV windows\n",
    "\n",
    "display(detections_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e6290",
   "metadata": {},
   "source": [
    "## Matching video frames and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "064e7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gaze_in_bbox(gaze_x, gaze_y, x_min, y_min, x_max, y_max):\n",
    "    return x_min <= gaze_x <= x_max and y_min <= gaze_y <= y_max\n",
    "\n",
    "'''def drop_person_if_other_present(group):\n",
    "    if \"person\" in group[\"Class\"].values and len(group[\"Class\"].unique()) > 1:\n",
    "        return group[group[\"Class\"] != \"person\"]\n",
    "    return group'''\n",
    "\n",
    "def filter_objects_ny_area(group):\n",
    "    group = group.copy()\n",
    "    group[\"area\"] = (group[\"X_max\"] - group[\"X_min\"]) * (group[\"Y_max\"] - group[\"Y_min\"])\n",
    "\n",
    "    # Find the minimum area\n",
    "    min_area = group[\"area\"].min()\n",
    "    \n",
    "    # Get all objects with the minimum area\n",
    "    smallest_objects = group[group[\"area\"] == min_area]\n",
    "    \n",
    "    if len(smallest_objects) == 1:\n",
    "        return smallest_objects\n",
    "    else:\n",
    "        person_objects = smallest_objects[smallest_objects[\"Class\"] == \"person\"]\n",
    "        if not person_objects.empty:\n",
    "            # Return only one person object (the first one)\n",
    "            return person_objects.iloc[[0]]\n",
    "\n",
    "    # If no person among smallest, return just one of the smallest objects\n",
    "    return smallest_objects.iloc[[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3954b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section id</th>\n",
       "      <th>recording id</th>\n",
       "      <th>fixation id</th>\n",
       "      <th>start timestamp [ns]</th>\n",
       "      <th>end timestamp [ns]</th>\n",
       "      <th>duration [ms]</th>\n",
       "      <th>fixation x [px]</th>\n",
       "      <th>fixation y [px]</th>\n",
       "      <th>azimuth [deg]</th>\n",
       "      <th>elevation [deg]</th>\n",
       "      <th>timestamp [ns]</th>\n",
       "      <th>X_min</th>\n",
       "      <th>Y_min</th>\n",
       "      <th>X_max</th>\n",
       "      <th>Y_max</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Class</th>\n",
       "      <th>in_bbox</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9b5a0d1f-12c2-4988-9998-7b6b38f14f85</td>\n",
       "      <td>3580a3f9-c0d9-4d8f-9fc1-da7ef301db97</td>\n",
       "      <td>4</td>\n",
       "      <td>1746785095341716784</td>\n",
       "      <td>1746785095461850784</td>\n",
       "      <td>120</td>\n",
       "      <td>843.471</td>\n",
       "      <td>1025.316</td>\n",
       "      <td>1.534394</td>\n",
       "      <td>-26.814540</td>\n",
       "      <td>1746785095341716784</td>\n",
       "      <td>820.516479</td>\n",
       "      <td>69.223709</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1190.921387</td>\n",
       "      <td>0.517064</td>\n",
       "      <td>person</td>\n",
       "      <td>True</td>\n",
       "      <td>8.743449e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9b5a0d1f-12c2-4988-9998-7b6b38f14f85</td>\n",
       "      <td>3580a3f9-c0d9-4d8f-9fc1-da7ef301db97</td>\n",
       "      <td>5</td>\n",
       "      <td>1746785095551969784</td>\n",
       "      <td>1746785095732091784</td>\n",
       "      <td>180</td>\n",
       "      <td>784.365</td>\n",
       "      <td>889.247</td>\n",
       "      <td>-2.451384</td>\n",
       "      <td>-18.107890</td>\n",
       "      <td>1746785095551969784</td>\n",
       "      <td>762.237244</td>\n",
       "      <td>22.584534</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1196.884033</td>\n",
       "      <td>0.734929</td>\n",
       "      <td>person</td>\n",
       "      <td>True</td>\n",
       "      <td>9.837844e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9b5a0d1f-12c2-4988-9998-7b6b38f14f85</td>\n",
       "      <td>3580a3f9-c0d9-4d8f-9fc1-da7ef301db97</td>\n",
       "      <td>6</td>\n",
       "      <td>1746785095747103784</td>\n",
       "      <td>1746785095822216784</td>\n",
       "      <td>75</td>\n",
       "      <td>769.611</td>\n",
       "      <td>922.684</td>\n",
       "      <td>-3.458678</td>\n",
       "      <td>-20.239180</td>\n",
       "      <td>1746785095747103784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.021393</td>\n",
       "      <td>934.661865</td>\n",
       "      <td>1198.393555</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>person</td>\n",
       "      <td>True</td>\n",
       "      <td>1.117269e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>9b5a0d1f-12c2-4988-9998-7b6b38f14f85</td>\n",
       "      <td>3580a3f9-c0d9-4d8f-9fc1-da7ef301db97</td>\n",
       "      <td>7</td>\n",
       "      <td>1746785095912216784</td>\n",
       "      <td>1746785096107465784</td>\n",
       "      <td>195</td>\n",
       "      <td>809.119</td>\n",
       "      <td>992.592</td>\n",
       "      <td>-0.832406</td>\n",
       "      <td>-24.721621</td>\n",
       "      <td>1746785095912216784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.963669</td>\n",
       "      <td>1203.208618</td>\n",
       "      <td>1195.463623</td>\n",
       "      <td>0.269487</td>\n",
       "      <td>person</td>\n",
       "      <td>True</td>\n",
       "      <td>1.431217e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9b5a0d1f-12c2-4988-9998-7b6b38f14f85</td>\n",
       "      <td>3580a3f9-c0d9-4d8f-9fc1-da7ef301db97</td>\n",
       "      <td>8</td>\n",
       "      <td>1746785096152465784</td>\n",
       "      <td>1746785096407715784</td>\n",
       "      <td>255</td>\n",
       "      <td>801.852</td>\n",
       "      <td>1011.751</td>\n",
       "      <td>-1.337943</td>\n",
       "      <td>-25.946193</td>\n",
       "      <td>1746785096152465784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>807.322388</td>\n",
       "      <td>1197.517822</td>\n",
       "      <td>0.351183</td>\n",
       "      <td>person</td>\n",
       "      <td>True</td>\n",
       "      <td>9.667829e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              section id  \\\n",
       "26  9b5a0d1f-12c2-4988-9998-7b6b38f14f85   \n",
       "28  9b5a0d1f-12c2-4988-9998-7b6b38f14f85   \n",
       "32  9b5a0d1f-12c2-4988-9998-7b6b38f14f85   \n",
       "37  9b5a0d1f-12c2-4988-9998-7b6b38f14f85   \n",
       "40  9b5a0d1f-12c2-4988-9998-7b6b38f14f85   \n",
       "\n",
       "                            recording id  fixation id  start timestamp [ns]  \\\n",
       "26  3580a3f9-c0d9-4d8f-9fc1-da7ef301db97            4   1746785095341716784   \n",
       "28  3580a3f9-c0d9-4d8f-9fc1-da7ef301db97            5   1746785095551969784   \n",
       "32  3580a3f9-c0d9-4d8f-9fc1-da7ef301db97            6   1746785095747103784   \n",
       "37  3580a3f9-c0d9-4d8f-9fc1-da7ef301db97            7   1746785095912216784   \n",
       "40  3580a3f9-c0d9-4d8f-9fc1-da7ef301db97            8   1746785096152465784   \n",
       "\n",
       "     end timestamp [ns]  duration [ms]  fixation x [px]  fixation y [px]  \\\n",
       "26  1746785095461850784            120          843.471         1025.316   \n",
       "28  1746785095732091784            180          784.365          889.247   \n",
       "32  1746785095822216784             75          769.611          922.684   \n",
       "37  1746785096107465784            195          809.119          992.592   \n",
       "40  1746785096407715784            255          801.852         1011.751   \n",
       "\n",
       "    azimuth [deg]  elevation [deg]       timestamp [ns]       X_min  \\\n",
       "26       1.534394       -26.814540  1746785095341716784  820.516479   \n",
       "28      -2.451384       -18.107890  1746785095551969784  762.237244   \n",
       "32      -3.458678       -20.239180  1746785095747103784    0.000000   \n",
       "37      -0.832406       -24.721621  1746785095912216784    0.000000   \n",
       "40      -1.337943       -25.946193  1746785096152465784    0.000000   \n",
       "\n",
       "        Y_min        X_max        Y_max  Confidence   Class  in_bbox  \\\n",
       "26  69.223709  1600.000000  1190.921387    0.517064  person     True   \n",
       "28  22.584534  1600.000000  1196.884033    0.734929  person     True   \n",
       "32   3.021393   934.661865  1198.393555    0.268208  person     True   \n",
       "37   5.963669  1203.208618  1195.463623    0.269487  person     True   \n",
       "40   0.000000   807.322388  1197.517822    0.351183  person     True   \n",
       "\n",
       "            area  \n",
       "26  8.743449e+05  \n",
       "28  9.837844e+05  \n",
       "32  1.117269e+06  \n",
       "37  1.431217e+06  \n",
       "40  9.667829e+05  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## world_timestamps.csv file from pupil cloud represents timestamps for each video frame\n",
    "# merge_asof is like a nearest match join for ordered time-series data\n",
    "\n",
    "# renaming so all timestamp columns are named the same to merge the dfs\n",
    "fixations_df[\"timestamp [ns]\"] = fixations_df[\"start timestamp [ns]\"]\n",
    "\n",
    "## making a video_df which contains matching frames and ts\n",
    "ts = world_timestamps_df['timestamp [ns]']\n",
    "\n",
    "video_df = pd.DataFrame({\n",
    "        \"Frame\": np.arange(len(world_timestamps_df)),\n",
    "        \"timestamp [ns]\": ts,\n",
    "    })\n",
    "\n",
    "# Make sure everything is sorted by timestamp so merge_asof works properky\n",
    "fixations_df = fixations_df.sort_values(\"start timestamp [ns]\")\n",
    "world_timestamps_df = world_timestamps_df.sort_values(\"timestamp [ns]\")\n",
    "\n",
    "# Merge-asof to assign the closest frame_id to each fixation\n",
    "fixations_with_frame = pd.merge_asof(\n",
    "    fixations_df,\n",
    "    video_df,\n",
    "    on=\"timestamp [ns]\",\n",
    "    direction=\"nearest\",  # to match the closest timestamp from video_df, whether it's before or after the fixation timestamp\n",
    ") \n",
    "# merging to assign fixations to frames\n",
    "fixations_with_detections = pd.merge(\n",
    "    fixations_with_frame,\n",
    "    detections_df,\n",
    "    on=\"Frame\",\n",
    "    how=\"left\" # keep all the fixations, even if a frame has no detection => nan\n",
    ")\n",
    "\n",
    "# calculate in_bbox \n",
    "fixations_with_detections[\"in_bbox\"] = fixations_with_detections.apply(\n",
    "    lambda row: is_gaze_in_bbox(\n",
    "        row[\"fixation x [px]\"], row[\"fixation y [px]\"],\n",
    "        row[\"X_min\"], row[\"Y_min\"], row[\"X_max\"], row[\"Y_max\"]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# filter only fixations that match at least one bounding box\n",
    "in_bbox_fixations = fixations_with_detections[fixations_with_detections[\"in_bbox\"] == True]\n",
    "\n",
    "#  group by fixation ID and drop \"person\" if there is another class in same fixation group\n",
    "filtered_fixations = in_bbox_fixations.groupby(\n",
    "    \"Frame\", group_keys=False # grouping on frame as i need fixation id column later\n",
    ").apply(filter_objects_ny_area, include_groups=False)\n",
    "\n",
    "fixations_on_persons = filtered_fixations.loc[filtered_fixations[\"Class\"] == \"person\"]\n",
    "fixations_on_persons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043cc7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person', 'cell phone', 'baseball glove', 'bus', 'bicycle',\n",
       "       'suitcase', 'horse', 'skateboard', 'remote', 'potted plant',\n",
       "       'train', 'backpack', 'truck', 'handbag', 'car', 'parking meter',\n",
       "       'umbrella', 'clock', 'bench', 'surfboard', 'airplane'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_fixations[\"Class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca371ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_fixations[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8adbd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixations_on_persons = fixations_on_persons[fixations_on_persons[\"in_bbox\"] == True]\n",
    "\n",
    "categorized_fixations_all = []\n",
    "\n",
    "for _, row in face_df.iterrows():\n",
    "    timestamp = row['start timestamp [ns]']\n",
    "    fixation_id = row['fixation id']\n",
    "    fix_duration = (row['end timestamp [ns]'] - row['start timestamp [ns]'])/1000000 #converting to ms\n",
    "    fixation_type = \"background\" #make default type\n",
    "\n",
    "    # face fixations\n",
    "    if row.get('fixation on face') == True:\n",
    "        fixation_type = \"face\"\n",
    "\n",
    "    # body fixations (only if not already labeled as face)\n",
    "    elif not fixations_on_persons[fixations_on_persons['timestamp [ns]'] == timestamp].empty:\n",
    "        fixation_type = \"body\"\n",
    "        matching_row = fixations_on_persons[fixations_on_persons['timestamp [ns]'] == timestamp].iloc[0]\n",
    "        fixation_id = matching_row['fixation id']\n",
    "        fix_duration = matching_row['duration [ms]']\n",
    "\n",
    "    categorized_fixations_all.append({\n",
    "        'timestamp [ns]': timestamp,\n",
    "        'type': fixation_type,\n",
    "        'fixation id': fixation_id,\n",
    "        'duration [ms]': fix_duration\n",
    "    })\n",
    "\n",
    "# a new df\n",
    "all_categorized_fixations_df = pd.DataFrame(categorized_fixations_all)\n",
    "saving_path = os.path.join(recording_path, f\"fixations_on_everything_{participant_number}_{recording_number}.csv\")\n",
    "all_categorized_fixations_df.to_csv(saving_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2efce969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_categorized_fixations_df[70:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9719febd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## comparing with only person detections\\ncategorized_fixations_file = os.path.join(recording_path, f\"categorized_fixations_{participant_number}_{recording_number}.csv\")\\ncategorised_fixations_df = pd.read_csv(categorized_fixations_file)\\n\\nn_face_fixations = len(categorised_fixations_df[categorised_fixations_df[\\'type\\'] == \\'face\\'])\\nn_body_fixations = len(categorised_fixations_df[categorised_fixations_df[\\'type\\'] == \\'body\\'])\\nn_background_fixations = len(categorised_fixations_df[categorised_fixations_df[\\'type\\'] == \\'background\\'])\\nprint(f\"Fixations - Face: {n_face_fixations}, Body: {n_body_fixations}, Background: {n_background_fixations}\")\\n\\nn_face = len(all_categorized_fixations_df[all_categorized_fixations_df[\\'type\\'] == \\'face\\'])\\nn_body = len(all_categorized_fixations_df[all_categorized_fixations_df[\\'type\\'] == \\'body\\'])\\nn_back = len(all_categorized_fixations_df[all_categorized_fixations_df[\\'type\\'] == \\'background\\'])\\n\\nprint(f\"Fixations - Face: {n_face}, Body: {n_body}, Background: {n_back}\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## comparing with only person detections\n",
    "categorized_fixations_file = os.path.join(recording_path, f\"categorized_fixations_{participant_number}_{recording_number}.csv\")\n",
    "categorised_fixations_df = pd.read_csv(categorized_fixations_file)\n",
    "\n",
    "n_face_fixations = len(categorised_fixations_df[categorised_fixations_df['type'] == 'face'])\n",
    "n_body_fixations = len(categorised_fixations_df[categorised_fixations_df['type'] == 'body'])\n",
    "n_background_fixations = len(categorised_fixations_df[categorised_fixations_df['type'] == 'background'])\n",
    "print(f\"Fixations - Face: {n_face_fixations}, Body: {n_body_fixations}, Background: {n_background_fixations}\")\n",
    "\n",
    "n_face = len(all_categorized_fixations_df[all_categorized_fixations_df['type'] == 'face'])\n",
    "n_body = len(all_categorized_fixations_df[all_categorized_fixations_df['type'] == 'body'])\n",
    "n_back = len(all_categorized_fixations_df[all_categorized_fixations_df['type'] == 'background'])\n",
    "\n",
    "print(f\"Fixations - Face: {n_face}, Body: {n_body}, Background: {n_back}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4de0b",
   "metadata": {},
   "source": [
    "## For epochs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09b76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For triggers ##\n",
    "fixations_on_persons = fixations_with_detections[fixations_with_detections[\"in_bbox\"] == True]\n",
    "\n",
    "categorized_fixations_triggers = []\n",
    "\n",
    "for _, row in face_df.iterrows():\n",
    "    timestamp = row['start timestamp [ns]']\n",
    "    fixation_type = \"background\"\n",
    "\n",
    "    # face fixations\n",
    "    if row.get('fixation on face') == True:\n",
    "        fixation_type = \"face\"\n",
    "\n",
    "    # body fixations (only if not already labeled as face)\n",
    "    elif not fixations_on_persons[fixations_on_persons['timestamp [ns]'] == timestamp].empty:\n",
    "        fixation_type = \"body\"\n",
    "        matching_row = fixations_on_persons[fixations_on_persons['timestamp [ns]'] == timestamp].iloc[0]\n",
    "\n",
    "    categorized_fixations_triggers.append({\n",
    "        'timestamp [ns]': timestamp,\n",
    "        'type': fixation_type,\n",
    "    })\n",
    "\n",
    "# a new df\n",
    "categorized_fixations_triggers_df = pd.DataFrame(categorized_fixations_triggers)\n",
    "saving_path = os.path.join(recording_path, f\"for_epochs_{participant_number}_{recording_number}.csv\")\n",
    "categorized_fixations_triggers_df.to_csv(saving_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
